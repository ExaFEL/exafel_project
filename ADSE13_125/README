Scripts for running IOTA demo on Cori KNL [Dec 2018]

Initial scripts courtesy Mona <monarin@stanford.edu>.
Contact: Asmit Bhowmick <abhowmick@lbl.gov>
         Nick Sauter <nksauter@lbl.gov>


For all the steps below, the workspace where you will be running jobs is called $BASE_PATH
Prior to following the steps below, make sure you have access to the raw XTC data. The data is available on NERSC at
/global/cscratch1/sd/psdatmgr/data/psdm/cxi/cxic0415/demo/xtc/

Within that folder above, there is a bigdata file cxic0415.xtc (2.1 Tb) and a smalldata folder. Create a folder in your work directory called $BASE_PATH/xtc2. Soft link the bigdata file as test-r0050.xtc and copy over the smalldata folder into that xtc2  folder. All the runs from cxic0415 are glued together in this one xtc2 file and is named run 50 (test-r0050.xtc) so that all the calibration constants can be picked up correctly.

##### STEPS FOR IOTA DEMO ########

1. Please use the Dockerfile in docker_psana2 in the exafel_project repository to build your own docker/shifter image. 
   If you wish to use the one for the demo on Cori-PII, it is registry.services.nersc.gov/asmit/iota_ps2_v2:latest 

2. Copy over this folder to a location where you wish to process data. That location is your $BASE_PATH in the submit
   scripts provided here.

3. Next decide which filesystem you would like to read in data from and read out data to. If anything involves burst
   buffer, use the processing/burst_buffer scripts. Otherwise use the scripts in processing/lustre_fs. Please do not 
   try this demo on the GPFS system. Read the instructions very carefully in the submission script as well as the 
   associated script that is run by that submission script (see point 4)

4. Following are the 4 types of cases for which submission scripts have been provided

   Submission script               |       script that is run       |  Indexing algorithm  |    Location

 a)  sbatch.sh 				docker_xtc_process.sh   	Conventional	     processing/lustre_fs
 b)  sbatch_iota.sh (used for DEMO)     docker_xtc_process_iota.sh      IOTA		     processing/lustre_fs
 c)  sbatch_bb.sh                       docker_xtc_process_bb.sh        Conventional         processing/burst_buffer
 d)  sbatch_iota_bb.sh                  docker_xtc_process_iota_bb.sh   IOTA                 processing/burst_buffer


5. The IOTA/Conventional indexing codes that were used for the project have been provided in processing/command_line. The xtc_process.py script is from cctbx_project/xfel/command_line. The xtc_process_iota_srs.py script is from exafel_project/ADSE13_25/command_line. The scripts provided here represent the state they were at the time of the demo.


5. You will have to change the DATA_DIR variable in the docker_xtc_process_**.sh script which you choose to use.
   This DATA_DIR points to where the xtc2 streams are.


6. Submit the submission scripts using the sbatch command to the Cori queue.
   Example is in processing/lustre_fs ==> sbatch sbatch_iota.sh


7. Once the job is over, you will have to run the indexing analytics package to figure out performance metrics
   We used a Standard Reporting format for this project that allows us to report timings as well as scientific
   performance and compare it across different situations. To generate the relevant table, please run the script
   processing/lustre_fs/sbatch_idx_analytics.sh. This can be run for jobs run on the burst buffer as well after data has   been staged out completely. Things to change in that script
     o change the sources via the variable CCTBX_XFEL
     o wall_time ==> time in seconds it took for the processing job in (6) to finish 
     o num_nodes ==> Number of nodes used for running the job in (6)

8. Merging step --> Please look at scripts in merging/. There are 3 steps in order to merge the data and get the MTZ file - 

  a) CXI_MERGE: First we merged each tar file (written out by individual ranks in xtc_process or xtc_process_iota_srs) separately. These jobs are run using the script merge.sh which has all the merging parameters. For the IOTA demo, we used the Ha14 error model available in CXI_MERGE. All the merging jobs can be submitted using submit_all.sh. I tried to process 1000 tar files on a single KNL node in one 24 hour job - the submit_all.sh script reflects that. The tag used is cxic0415_$n where n is rank number in the integrated pickle file. 
  o Script to submit job --> merging/submission_scripts/submit_all.sh
  o Script that has the merging parameters and does the work --> merging/submission_scripts/merge.sh

  b) CONCATENATE DB FILES: Once all the merging jobs have finished, we should have a .db file for each tar file that was processed. Before running CXI_XMERGE, we need to concatenate these database file. This is done using the script merging/concatenate_merging_dbs.py. This job can be run on Cori-PII using sbatch_concatenate.sh. The new concatenated database files have the tag cxic0415_all  
  o script to submit job --> merging/submission_scripts/sbatch_concatenate.sh
  o script that does the main work underneath --> merging/concatenate_merging_dbs.py

  c) CXI_XMERGE: Finally in order to get statistics like CC1/2, Multiplicity, we have to run CXI_XMERGE. This can be run using the xmerge.sh script. The final xmerge step is quite memory intensive and I had to run it on DIALS.LBL.GOV. This can be done by just copying over the cxic0415_all_* database files 
  o script to submit job --> merging/submission_scripts/sbatch_xmerge.sh
  o script that has the parameters and does the work --> merging/submission_scripts/xmerge.sh

9. Statistics from merging: CXI_XMERGE generates the MTZ file for the full dataset  along with MTZ files for the half datasets. Here is what you need to do for each of the metrics reported. l

  o CC1/2 --> Read it off the cxic0415_all_mark0.log 
  o Multiplicity --> Same as above
  o CCano --> Run the script cc_anom_calculator and pass in the 2 half dataset MTZ files as arguments. Make sure you have sourced CCTBX_XFEL
